{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Conditional Samples (GPT-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install Required Libraries\n",
    "!pip3 install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching checkpoint: 1.00kit [00:00, 961kit/s]                                                      \n",
      "Fetching encoder.json: 1.04Mit [00:00, 38.8Mit/s]                                                   \n",
      "Fetching hparams.json: 1.00kit [00:00, 281kit/s]                                                    \n",
      "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:10, 49.2Mit/s]                                  \n",
      "Fetching model.ckpt.index: 6.00kit [00:00, 1.47Mit/s]                                               \n",
      "Fetching model.ckpt.meta: 472kit [00:00, 11.5Mit/s]                                                 \n",
      "Fetching vocab.bpe: 457kit [00:00, 33.6Mit/s]                                                       \n"
     ]
    }
   ],
   "source": [
    "#Download the models\n",
    "#Model Options: 117M or 345M\n",
    "!python3 download_model.py 117M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model, sample, encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Interactive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interact_model(\n",
    "    model_name='117M',\n",
    "    seed=None,\n",
    "    nsamples=1,\n",
    "    batch_size=1,\n",
    "    length=None,\n",
    "    temperature=1,\n",
    "    top_k=0,\n",
    "    models_dir='models',    \n",
    "):\n",
    "    \"\"\"\n",
    "    Interactively run the model\n",
    "    :model_name=117M : String, which model to use\n",
    "    :seed=None : Integer seed for random number generators, fix seed to reproduce\n",
    "     results\n",
    "    :nsamples=1 : Number of samples to return total\n",
    "    :batch_size=1 : Number of batches (only affects speed/memory).  Must divide nsamples.\n",
    "    :length=None : Number of tokens in generated text, if None (default), is\n",
    "     determined by model hyperparameters\n",
    "    :temperature=1 : Float value controlling randomness in boltzmann\n",
    "     distribution. Lower temperature results in less random completions. As the\n",
    "     temperature approaches zero, the model will become deterministic and\n",
    "     repetitive. Higher temperature results in more random completions.\n",
    "    :top_k=0 : Integer value controlling diversity. 1 means only 1 word is\n",
    "     considered for each step (token), resulting in deterministic completions,\n",
    "     while 40 means 40 words are considered at each step. 0 (default) is a\n",
    "     special setting meaning no restrictions. 40 generally is a good value.\n",
    "     :models_dir : path to parent folder containing model subfolders\n",
    "     (i.e. contains the <model_name> folder)     \n",
    "    \"\"\"\n",
    "    models_dir = os.path.expanduser(os.path.expandvars(models_dir))\n",
    "    if batch_size is None:\n",
    "        batch_size = 1\n",
    "    assert nsamples % batch_size == 0\n",
    "\n",
    "    enc = encoder.get_encoder(model_name, models_dir)\n",
    "    hparams = model.default_hparams()\n",
    "    with open(os.path.join(models_dir, model_name, 'hparams.json')) as f:\n",
    "        hparams.override_from_dict(json.load(f))\n",
    "\n",
    "    if length is None:\n",
    "        length = hparams.n_ctx // 2\n",
    "    elif length > hparams.n_ctx:\n",
    "        raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
    "\n",
    "    with tf.Session(graph=tf.Graph()) as sess:\n",
    "        context = tf.placeholder(tf.int32, [batch_size, None])\n",
    "        np.random.seed(seed)\n",
    "        tf.set_random_seed(seed)\n",
    "        output = sample.sample_sequence(\n",
    "            hparams=hparams, length=length,\n",
    "            context=context,\n",
    "            batch_size=batch_size,\n",
    "            temperature=temperature, top_k=top_k\n",
    "        )\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        ckpt = tf.train.latest_checkpoint(os.path.join(models_dir, model_name))\n",
    "        saver.restore(sess, ckpt)\n",
    "\n",
    "        while True:\n",
    "            raw_text = input(\"Model prompt >>> \")\n",
    "            while not raw_text:\n",
    "                print('Prompt should not be empty!')\n",
    "                raw_text = input(\"Model prompt >>> \")\n",
    "            context_tokens = enc.encode(raw_text)\n",
    "            generated = 0\n",
    "            for _ in range(nsamples // batch_size):\n",
    "                out = sess.run(output, feed_dict={\n",
    "                    context: [context_tokens for _ in range(batch_size)]\n",
    "                })[:, len(context_tokens):]\n",
    "                for i in range(batch_size):\n",
    "                    generated += 1\n",
    "                    text = enc.decode(out[i])\n",
    "                    print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n",
    "                    print(text)\n",
    "            print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/117M/model.ckpt\n",
      "Model prompt >>> This is a sample model\n",
      "======================================== SAMPLE 1 ========================================\n",
      " in which most instruments, turbo cars and transportation equipment are hard to place in a safe location. Because every part has a little bit of risk, it is always best to opt for a compact model for full context — for the overload risk less, and the less visible it is, the better.\n",
      "\n",
      "However, in the real world, where in the past many people with big potential were safest stockpiled in selecting a high master control unit, these days the landing jumbo field horn and other equipment is reserved for highly established airframes, even to the tip of the nose, and it has become more popular for small, bright 50s hosiery trucks.\n",
      "\n",
      "One way to find out what makes people afraid is to visit racebook, where you can find real studies on how people feel when things are going well and by whom directly, but it also means looking for the desired height, weight and difficulty, one less thing that makes them terrified when you say it.\n",
      "\n",
      "4. \"Understanding Champion Imperial Craft•Warden's Base Ordnance\"\n",
      "\n",
      "Literally say, \"Oh yes, I can work in novices.\"\n",
      "\n",
      "The value of that setting in this post will be standard \"problems\" mentioned in our introduction. Moving beyond technology and necessities will probably make this easier. Like so many others who come across these models in other domains where we are suggesting using test duty for sure will — with politely \"Organ Stopped crop Streaking friend\".\n",
      "\n",
      "To put it simply: \"Metapci out of the Valley. And the Elders insist they hoist a head blown off it. I'd rather than know for sure what it means. But hey, no worries, if my teeth add up in a month or two, poor RM replaces me outright.\" Hold it for the moment, baby. And there it is. Generally, the more tacit understanding of the mechanic and what can work, the more their wand becomes spent zapped into myths about things like aircraft coming out of throttle and accelerating, the peace of mind a sentence can bring.\n",
      "\n",
      "Since these model mechanics and the fleets off into BOS don't make a fancy immune to anthropomorphism or irrationalism in pursuit of maximizing win conditions, you are pretty much checking what lightly tunes employment methods endear you — color-blind explanations of attributes, helmets, and bunker blast tickets for name brand commercial vehicles like, for example, tiny intercontinental ballistic missiles. (The ebb and flow of your hot school girls fighting in your clairvoy\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "interact_model(\n",
    "    model_name='117M',\n",
    "    seed=None,\n",
    "    nsamples=1,\n",
    "    batch_size=1,\n",
    "    length=None,\n",
    "    temperature=1,\n",
    "    top_k=0,\n",
    "    models_dir='models',    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
